{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# DATA SCIENCE AND MACHINE LEARNING",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**CONTENTS -**\n\nPART - A - CONCEPTS\n\nPART - B - PYTHON\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. **Data Science Concept** -The Thought of Data Science Came from Data Analysis. Data Analysis isn't new. Statisticians have been doing it for many years. Even Normal people have been Doing it in excel for years.But Slowly amount of data increased due to Wide Internet usage. It is become impossible for statistician to analyze such huge quantity of data. So various software and programing languages have been developed for doing this Analysis. Analysis of data through this softwares and derive insights from it called Data Science. Also In Statistics we deal with samples of data, To find insights from it. But in Data Science we deal with total data. So Unlike Statistics in data Science insights are more accurate.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "2. **Data Mining** - ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "a. **Establishing Goals**\n- Identify Key Questions That to be Answered\n- Expected level of acquiricy of the result.\n- Higher the expected Level of Acquiricy Higher the Project cost\n- Usefullness of the result\n\nb. **Selecting Data**\n- The output  largely depends upon the quality of data being used. \n- If data is not readily available plan new data collection initiatives, including surveys.\n- Availability of Data Also effecting Cost\n\nc. **Preprocessing Data**\n- STEP 1 - Identify the irrelevant attributes and erroneous aspects of data to ensure integrity. \n- STEP 2 - Develop a formal method of dealing with missing data in advance and determine whether the data are missing randomly or systematically  and its impact on results.If observations or variables are missing randomly its impact is less, if data is missing systamaticaly This would lead to systematic biases in the analysis.\n- STEP 3 - After the relevant attributes of data have been retained, the next step is to determine the appropriate format in which data must be stored. \n- STEP 4 -  Reduce the number of attributes needed to explain the phenomena by transforming observations or variables reduction algorithms such as Principal Component Analysis without a significant loss in information. Often you need to transform variables from one type to another. Like Continious Variable to Categorial Variable. This could help capture the non-linearities in the underlying behaviors.\n\nd. **Storing Data**\n- Transformed data must be stored in a format that makes it conducive for data mining. \n- Data must be stored in a format that gives unrestricted and immediate read/write privileges to the data scientist. \n- During data mining, new variables are created, which are written back to the original database, which is why the data storage scheme should facilitate efficiently reading from and writing to the database. \n- It is also important to store data on servers or storage media that keeps the data secure and also prevents the data mining algorithm from unnecessarily searching for pieces of data scattered on different servers or storage media. Data safety and privacy should be a prime concern for storing data.\n\ne. **Mining Data** - Finding Trends - Finding Sonething - Data Analysis\n- Data analysis using, including parametric and non-parametric methods, and machine-learning algorithms. A good starting point for data mining is data visualization. \n- Multidimensional views of the data using the advanced graphing capabilities of data mining software are very helpful in developing a preliminary understanding of the trends hidden in the data set. \n\nf. **Evaluating Mining Results**\n- In-sample forecast means testing the predictive capabilities of the models on observed data to see how effective and efficient the algorithms have been in reproducing data. \n- Evalution of results in the light of the Stakeholders feedback and use better and improved algorithms to improve the quality of results. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "3. **The Report Structure**\n- **Detailed Report or Brief Report** - Before starting the analysis, think about the structure of the report. It can be a 5 page report or  100 page document. A brief report is more to the point and presents a summary of key findings.\nA detailed report incrementally builds the argument and contains details about other relevant works, research methodology, data sources, and intermediate findings along with the main results.\nThe length of the reports varied depending largely on the purpose of the report. Brief reports were drafted as commentaries on current trends and developments that attracted public or media attention.\nDetailed and comprehensive reports offered a critical review of the subject matter with extensive data analysis and commentary.\nOften, detailed reports collected new data or interviewed industry experts to answer the research questions. Even if you expect the report to be brief, sporting five or fewer pages, I recommend that the deliverable follow a prescribed\nformat including the cover page, table of contents, executive summary, detailed contents, acknowledgments, references, and appendices (if needed).\n- **Cover Report** - I often find the cover page to be missing in documents. It is not the inexperience of undergraduate students that is reflected in submissions that usually miss the cover page.\nIn fact, doctoral candidates also require an explicit reminder to include an informative cover page. I hasten to mention that the business world sleuths are hardly any better.\nJust search the Internet for reports and you will find plenty of reports from reputed firms that are missing the cover page. At a minimum, the cover page should include the title of the report, names of authors, their affiliations,\nand contacts, the name of the institutional publisher (if any), and the date of publication. I have seen numerous reports missing the date of publication, making it impossible to cite them without the year\nand month of publication. Also, from a business point of view, authors should make it easier for the reader to reach out to them. Having contact details at the front makes the task easier.\n- **Table of Contents** - \"A table of contents (ToC)\" is like a map needed for a trip never taken before. You need to have a sense of the journey before embarking on it.\nA map provides a visual proxy for the actual travel with details about the landmarks that you will pass by in your trip. The ToC with main headings and lists of tables and figures offers a glimpse of what lies ahead in the document.\nNever shy away from including a ToC, especially if your document, excluding cover page, table of contents, and references, is five or more pages in length.\nEven for a short document, I recommend an \"abstract\" or an \"executive summary\". Nothing is more powerful than explaining the crux of your arguments in three paragraphs or less.\nOf course, for larger documents running a few hundred pages, the executive summary could be longer.\n- **Introductory Section** - An \"introductory section\" is always helpful in setting up the problem for the reader who might be new to the topic and who might need to be gently introduced to the subject matter\nbefore being immersed in intricate details. A good follow-up to the introductory section is a review of available relevant research on the subject matter.\nThe length of the literature review section depends upon how contested the subject matter is. In instances where the vast majority of researchers have concluded in one direction,\n- **Nuanced arguments** - On the other hand, if the arguments are more nuanced with caveats aplenty, then you must cite the relevant research to offer adequate context before you embark on your analysis.\n- **Literature review**- You might use the literature review to highlight gaps in the existing knowledge, which your analysis will try to fill. This is where you formally introduce your research questions and hypothesis.\n- **Methodology** - In the \"methodology\" section, you introduce the research methods and data sources you used for the analysis. If you have collected new data, explain the data collection exercise in some detail.\nYou will refer to the literature review to bolster your choice for variables, data, and methods and how they will help you answer your research questions.\n- **Results** - The results section is where you present your empirical findings. Starting with descriptive statistics.and illustrative graphics you will move toward formally testing your hypothesis\nIn case you need to run statistical models, you might turn to regression models or categorical analysis. If you are working with time-series data\nYou can also report results from other empirical techniques that fall under the general rubric of data mining.\nNote that many reports in the business sector present results in a more palatable fashion by holding back the statistical details and relying on illustrative graphics to summarize the results.\nThe results section is followed by the discussion section, where you craft your main arguments by building on the results you have presented earlier.\n- **Discussion Knowledge Gaap**- The \"discussion section\" is where you rely on the power of narrative to enable numbers to communicate your thesis to your readers.\nYou refer the reader to the research question and the knowledge gaps you identified earlier. You highlight how your findings provide the ultimate missing piece to the puzzle.\nOf course, not all analytics return a smoking gun. At times, more frequently than I would like to acknowledge, the results provide only a partial answer to the question and that, too, with a long list of caveats.\n- **Conclusion** - In the \"conclusion\" section, you generalize your specific findings and take on a rather marketing approach to promote your findings so that the reader does not remain stuck in\nthe caveats that you have voluntarily outlined earlier. You might also identify future possible developments in research and applications that could result from your research.\nWhat remains is housekeeping, including a list of references, the acknowledgment section (acknowledging the support of those who have enabled your work is always good), and \"appendices\", if needed.\n- **Important Questions**\nHave You Done Your Job as a Writer? As a writer, you are responsible for communicating your findings to the readers.\nHave You done your Job as a Data Scientist? As a data scientist, you are expected to do thorough analysis with the appropriate data, deploying the appropriate tools.\nTransport Policy, a leading research publication in transportation planning, offers a checklist for authors interested in publishing with the journal.\nHave you told readers, at the outset, what they might gain by reading your paper?\nHave you made the aim of your work clear?\nHave you explained the significance of your contribution?\nHave you set your work in the appropriate context by giving sufficient background (including a complete set of relevant references) to your work?\nHave you addressed the question of practicality and usefulness?\nHave you identified future developments that might result from your work?\nHave you structured your paper in a clear and logical fashion?\n- **Leanth and Content** - The length and content of the final report will vary depending on the needs of the project.\nThe structure of the final report for a Data Science project should include a cover page, table of contents, executive summary, detailed contents, acknowledgements, references and appendices.\nThe report should present a thorough analysis of the data and communicate the project findings.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "3. **Data Science Methodology**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A. **Business Understanding And Analytical Approach**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- The need to understand and prioritize the business goal.\n- The way stakeholder support influences a project.\n- The importance of selecting the right model.\n- When to use a predictive, descriptive, or classification model. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "B. **Data Requirement And Data Collection**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- Identify data requirements for your model.\n- Why the content, format, and representation of your data matter.   \n- The importance of identifying the correct sources of data for your project.\n- How to handle unavailable and redundant data.\n- To anticipate the needs of future stages in the process.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "C. **Data Understanding And Preparation**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- The importance of descriptive statistics.\n- How to manage missing, invalid, or misleading data.\n- The need to clean data and sometimes transform it.\n- The consequences of bad data for the model.\n- Data understanding is iterative; you learn more about your data the more you study it. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "D. **Modeling And Evolution**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- The difference between descriptive and predictive models.\n- The role of training sets and test sets.\n- The importance of asking if the question has been answered.\n- Why diagnostic measures tools are needed.\n- The purpose of statistical significance tests.\n- That modeling and evaluation are iterative processes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "E. **Deployment And Feedback**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- The importance of stakeholder input.\n- To consider the scale of deployment.\n- The importance of incorporating feedback to refine the model.\n- The refined model must be redeployed.\n- This process should be repeated as often as necessary.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- **10 Questions to Ask**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. What is the problem that you are trying to solve?\n2. How can you use the Data to Answer the Question?\n3. What data do you need to answer the question?\n4. Where is the data comming from and how you get it?\n5. Is the data you have collected representative of the problem to be solved?\n6. What additional work is required to manipulate and work with the data?\n7. In what way can the data be visualized to get the answer that is required?\n8. Does the model used realy answer the initial question or does it need to be adjusted?\n9. Can you put the model into practice?\n10. Can you get constructive feedback to answering the question?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "4. **Machine learning**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Machine learning  is a subset of AI that uses computer algorithms to analyze data and make intelligent decisions based on what it is learned without being explicitly programmed.Machine learning algorithms are trained with large sets of data and they learn from examples. They do not follow rules-based algorithms. Machine learning is what enables machines to solve problems on their own and make accurate predictions using the provided data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Techniques** -",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. **Regression/Estimation** - Predecting Continioues values for example price of a house based on its characteristics or estimate co2 ommission from a car engine. \n2. **Classification** - Predecting class or catagory of a case.for example, if a cell is benign or malignant, or whether or not a customer will churn. \n3. **Clustering** - Groups of similar cases, for example, can find similar patients, or can be used for customer segmentation in the banking field.\n4. **Association** -  Technique is used for finding items or events that often co-occur, for example, grocery items that are usually bought together by a particular customer.\n5. **Anomaly detection**-  Is used to discover abnormal and unusual cases, for example, it is used for credit card fraud detection.\n6. **Sequence mining** -  Is used for predicting the next event, for instance, the click-stream in websites.\n7. **Dimension reduction** -  Is used to reduce the size of data.\n8. **Recommendation systems**- This associates people's preferences with others who have similar tastes, and recommends new items to them, such as books or movies. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "5. **Deep learning And Nural Networks**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "6. **Artificial intelligence**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "7. **Big Data**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "8. **Database**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "9. **Information Technology**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "10. **Cloud**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}